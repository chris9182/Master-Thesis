\documentclass[
	a4paper,
	english,
	twoside,
	openright,               
	11pt                            
	]{report}

\usepackage{textcomp}
\usepackage{listings}
\usepackage{pgfplots}

\usepackage[T1,OT1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{float}
\usepackage{chngcntr}
\usepackage{placeins}
\usepackage[ngerman,american]{babel}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage[justification=centering]{caption}
\usepackage{glossaries}
\graphicspath{ {./img/} }
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\setcounter{tocdepth}{3}
\pgfplotsset{compat=newest}

\begin{document}
\input{titlepage/titlepage}
\input{start/acknowledgements}

\pagenumbering{gobble}

\tableofcontents
 \cleardoublepage
%
%
% list of figures
\listoffigures
%\protect addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\cleardoublepage

% list of tables
\listoftables
%\protect addcontentsline{toc}{chapter}{Tabellenverzeichnis}
\cleardoublepage

% list of algorithms
%\listofalgorithmsLeMATo
%\protect addcontentsline{toc}{chapter}{Algorithmenverzeichnis}
\cleardoublepage

 \part{Introduction and Preliminaries}
 \pagenumbering{arabic}
   \setcounter{page}{1}
 \include{start/introduction}


\chapter{Related Work}\label{cha:related_work}
\section{Clustering Algorithms}

Clustering is the task of finding groups of points in data such that points within a group have some kind of relationship, while points in different groups do not relate. Defining how such a relationship can be expressed and how these groupings can be found are non-trivial tasks, for which reason a lot of research as been put towards this topic. Different ways of thinking about this problem, the expected results and the performance aspects of the algorithms resulted in a large number of different methods. 

 To summarize which methods are available many surveys were written over time \cite{7154919,1427769,7414675,surveyclustering}. Another aspect of clustering is which kinds of data can be clustered and even for more specific tasks like text clustering many methods are available \cite{5982288}, though these will no be mentioned further on as this paper only works with numeric data. Also, work has been put into optimizing clustering algorithms for different types of architectures and machines, e.g. for distributed systems \cite{6322592} or optimized for energy consumption and load balancing \cite{7586361}. Such aspects will also not be further considered as this paper and the created tool do not aim to beat existing methods in run-time or energy efficiency.

To give an overview over existing clustering algorithms Xu and Tian \cite{surveyclustering} generalize the idea of algorithms to nine categories. These categories are defined by which ideas the algorithms are based on, which are the following:
\begin{itemize}
  \item Partition
  \item Hierarchy
  \item Fuzzy Theory
  \item Distribution
  \item Density
  \item Graph Theory
  \item Grid
  \item Fractal Theory
  \item Model
\end{itemize}

Partition based methods aim to find central points in the data to represent clusters. Data points are then assigned to the central point closes to them and each central point forms a cluster. The most famous method in this group is k-Means which has been implemented in different ways, an example for a newer way for this was proposed by Kanungo et al. \cite{1017616}.

Hierarchy based methods assume at first that each point represents a cluster and merges points to new clusters according to some logic. The opposite can also be done, first assuming all points belong to one cluster and the spiting them into multiple clusters. When starting with each point in one cluster typical linking strategies are single link, joining clusters according to the distance between the closest points, average link, joining according to the average distance of all points from one cluster to the other, or maximum link, whereby the maximum distance between points of the clusters is considered.

Fuzzy Theory based methods do unlike other methods not produce a discrete value of label ownership but a value in the interval $[0,1]$ representing how strongly a point belongs to a cluster. The usual method to compute such an ownership value is to optimize toward an objective function and seeing the values as likelihood of belonging to a given cluster.

Distribution based methods aim to find clusters with the idea that any cluster should have its own mathematical distribution. In other words, each distribution found in the data should have a cluster with its generated points assigned to it. In most cases Gaussian distributions are assumed, reducing the complexity of the search.

Density based methods assume that points with a dense neighborhood should belong to a common cluster. Typical parameters for such clustering algorithms include the size of the neighborhood to consider and the number of points that should be found in their, such that the points are considered densely connected. Example algorithms for this are DBSCAN \cite{10.5555/3001460.3001507} and OPTICS \cite{10.1145/304181.304187}, which was introduced as an improvement of DBSCAN.

Graph Theory based methods work by regarding each point in the data as a vertex of a graph with their distances represent the edges. The algorithms then aim to partition the graph into $k$ sub-graphs each representing a cluster, whereby the edges connecting sub-graphs should have a large sum (of the distances) or small sum (of similarities).

Grid based methods look at the data space as a multidimensional grid. The resulting rectangular spaces are then used in combination with hierarchical and density based methods to bothe improve the result and the computational performance of the algorithms.

Fractal Theory based methods aim to optimize the intrinsic quality of the fractal dimension \cite{surveyclustering}. In this context fractal represents the geometry divisible into multiple parts with common characters with the whole \cite{surveyclustering}.

Model based methods work by first assuming a model for the data and then optimizing the clustering to reflect the choice of model. Many of those are based on statistical learning methods whereby a classification tree is built hierarchically under the assumption that the distribution for each attribute is independent \cite{surveyclustering}.

\subsection{New Clustering Algorithms}

Newer methods for clustering have also been proposed, either with completely novel ideas or by combining ideas from the previous section. To briefly summarize \cite{surveyclustering}, those algorithms are for example based on:

\begin{itemize}
  \item Kernel
  \item Ensemble (more on this in Section \ref{sec:consensus})
  \item Swarm Intelligence
  \item Quantum Theory
  \item Spectral Graph Theory
  \item Affinity Propagation
  \item Density and Distance
\end{itemize}

Additionally algorithms where developed for special data structures \cite{surveyclustering}. These include:

\begin{itemize}
  \item Spatial Data
  \item Data Streams
  \item Large-Scale Data
\end{itemize}

For a more in depth overview Xu and Tian \cite{surveyclustering} published a survey paper, on which the categories in this paper were based on.

%more on current development?
\section{Visual Frameworks}
Visual Frameworks are used to both allow the user to analyze data and empower him to produce better results. Often, when tackling a data analysis problem, the issue is not that there are no good methods for the problem, but just that the choice of which method produces a satisfying result is difficult. As looking at quality values and data tables does often not reveal the necessary information visualizations are needed.

\subsection{Visual Clustering Frameworks}
Visual clustering frameworks and tools aim to help users to perform clustering in a simpler way and to visualize results, such that they can make more educated decisions on which algorithms to use. The visualizations also make it easier to grasp how clusters are structured and how good the computed result is.

%%%%%%%%%%%%%%%%%%%%%%%%%%%

Clustervision \cite{Kwon2018ClustervisionVS}

VISTA \cite{VISTA} ClusterMap and user result evaluation

iVIBRATE \cite{10.1145/1148020.1148024} extension VISTA?

simple visualizations ELKI implementation \cite{10.14778/2824032.2824115}


Frameworks with specific data-sets in mind:

Propose: ``An Evolutionary and Visual Framework for Clustering of DNA Microarray Data'' \cite{DNAVis}, tool for clustering DNA data with visualization

Even Memes \cite{Dang2017}
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{other Visual Frameworks}

In addition to visual frameworks for clustering, visual parameter space analysis is also relevant, as finding the best parameters for a clustering algorithm may be difficult. For such use-cases Sedlmair et al. \cite{6876043} propose a framework which can help in finding parameters.

There are also few tools that implement additional visualizations for consensus clustering. An example for this is ConsensusClusterPlus \cite{10.1093/bioinformatics/btq170} which visualizes the co-association matrix created from the base clusterings. This matrix represents how often two points occur in a common cluster relative to the number of base clusterings. Looking at this as a heat map a clear block structure is supposed to indicate a good choice for the parameter $k$ which defines the number of clusters in the result partitioning. It also implements Consensus Cumulative Distribution Function (CDF) Plots and Delta Area Plots, showing the CDF of the consensus matrix for each parameter $k$ and relative change for consecutive values of $k$. This Delta Area Plot is supposed to allow the user to determine a value $k$ at which the relative increase is negligible, similar to the elbow-method. Additional plots are also available, further enabling the user to compare the quality of consensus results regarding the parameter $k$.

\section{Consensus Clustering}\label{sec:consensus}
Consensus clustering aims to combine multiple clustering solutions for the same data-set into one result. This result should be representative of the combined knowledge acquired by all base clusterings and therefor be more robust and accurate than the average result at least. As the consensus result should lie in the center of the solution space spanned by the base clusterings, if there are sufficiently many good clusterings it should be able to overcome problems arising from outliers. In literature the problem of consensus clustering is often found under different names, those being cluster ensemble \cite{BOONGOEN20181}, clustering aggregation \cite{Gionis2005ClusteringA} and ensemble of partitions \cite{ensemblepartitions}. For the evaluation of the quality of a result five main factors can be defined \cite{Ghaemi2009ASC,survey1}:\newline

\begin{table}[ht]

\begin{tabular}{ll}
	\textbf{Novelty} & finding results otherwise unattainable \\
	\textbf{Robustness} & having better average quality than input clusterings \\
	\textbf{Consistency}   & finding a result somehow similar to input clusterings \\
	\textbf{Stability} & being more noise resilient than simple clustering algorithms \\
	\textbf{Scalability} & applicability for large data-sets regarding run-time \\
	&
\end{tabular}
\caption{Quality factors for consensus clustering results}
\end{table}

Consensus clustering includes two parts. Firstly base clusterings must be generated, which can be done with different strategies \cite{BOONGOEN20181}. One may choose to perform multiple runs of the same algorithm with the same parameters but different random seeds, the same algorithm with different parameters or even multiple different algorithms. All of these possibilities can be while always clustering the full data, a different sample of the data points (Bootstrapping) \cite{bootstrapping} each run (e.g. 80\%) or even different subspaces of the data. After the generation of base clusterings, the next step is to combine those with a consensus function. In general, consensus functions can be separated into two groups, Object Co-occurrence (O-Co) methods and Median Partitioning (MP) methods \cite{survey1}. There exist a large number of methods in both of those groups, which can be categorized according to what ideas they are based on.

\subsection{Object Co-occurrence}
Object Co-occurrence (O-Co) based consensus functions look at how points relate to each other and uses this information to create a consensus result. If two points often occur in a common cluster of the base clusterings intuition would suggest that they should also appear in a common cluster in the final result. The same holds for points mostly clustered apart, indicating that they should not be clustered together by the consensus function. The different O-Co based methods formulate algorithms that aim to create a result reflecting these thoughts and usually run in a deterministic way, not seeing this task as an optimization problem. This contrasts the methodology of Median Partitioning methods as can be seen in the following section. To summarize the groups of O-Co consensus functions they are categorized by their underlying computational strategy. The different methods are based on:

\begin{itemize}
  \item Relabeling and Voting
  \item Co-association Matrix
  \item Link Uncertainty
  \item Dual-Similarity
  \item Spectral Methods
  \item Graph and Hypergraph
  \item Minimum Spanning Tree
  \item Locally Adaptive Clustering Algorithm
  \item Information Theory
  \item Finite Mixture Model
\end{itemize}

Relabeling and Voting based methods use the individual points as  ``voters'' whereby each point can vote for which cluster it should be assigned to \cite{4470298}. For this to work, the different base clusterings need to map their clusters to one another, defining which cluster is associated most closely with which cluster from another base clustering. This mapping can be performed with algorithms like Kuhnâ€™s Hungarian method \cite{Kuhn2010}.

Co-association Matrix based method create a matrix representing how often two points are in the same cluster relatively  \cite{Monti2003}. With this matrix the consensus result can be computed using linking of sets hierarchically or with a clustering algorithm seeing this matrix as a similarity matrix. The Link Uncertainty \cite{6413733} and Dual-Similarity \cite{7344797} methods expand on this idea by allowing uncertainty values in the matrix and using neighbor-neighbor relationships respectively. Spectral Methods also expand on the idea of co-association matrices by trying to remove noise from the matrix before creating the final result \cite{Tao:2016:RSE:2983323.2983745}.

Graph and Hypergraph based methods create graphs from the base clusterings, representing the relationship between the points as edges and split the resulting graph into sub-graphs with graph partitioning \cite{Strehl:2003:CEK:944919.944935}. Representative methods from this group are the Cluster-based Similarity Partitioning Algorithm (CSPA), the HyperGraph-Partitioning Algorithm (HGPA) and the Meta-CLustering Algorithm (MCLA). Minimum Spanning Tree algorithms require a specific graph structure, organizing them as a minimum spanning tree with Prim's Algorithm \cite[p.~276]{10.1007/978-3-540-85033-5_27}. From this tree the final result is derived through majority voting, a prominent method of this category is Divisive Clustering Ensemble with Automatic Cluster Number (DICLENS) \cite{6035671}.

Locally Adaptive Clustering Algorithm look at different subspaces of the data and computes weights from clustering results in these subspaces \cite{Domeniconi2007}. To create a result the computed clusterings and their weights are modeled as a graph, which is then partitioned.

Information Theory based methods compute quality measures to define distances between clusters and use k-Means to cluster those together \cite[p.~353]{survey1}.

Finite Mixture Model based methods define the consensus clustering problem similarly to the EM-Algorithm (Expectation Maximization) for clustering \cite{Goder2008ConsensusCA,Topchy2004AMM}. The labels for each point are defined as  probability distributions and the final solution is obtained by solving a maximum likelihood estimation problem.

\subsection{Median Partitioning}
The main difference of Median Partitioning in comparison to Object Co-occurrence is that methods of this group tackle consensus clustering as an optimization problem. With the distance between clusterings defined the task of finding a median partition is to look for a solution that would be in the center of the solution space spanned by the base clusterings. In other words the resulting clustering should have a minimal sum of distances to all base clusterings. As this problem has been shown to be NP-hard \cite{np_median_partition}, heuristics must be used. The different groups of methods can be roughly described as methods that are based on:

\begin{itemize}
  \item Genetic Algorithms
  \item Mirkin Distance
  \item Sum of Pairwise Distances
  \item Kernel Functions
  \item Non-negative Matrix Factorization
  \item Binary Linear Programming
\end{itemize}

Genetic Algorithm based methods require a distance measure and a fitness function to compute a consensus result \cite{Cristofor2002FindingMP}. Through evolutionary steps different, starting with random, results are combined or modified until the fitness of the result is satisfactory or does not further improve over many iterations. Mirkin Distance is often used as a distance measure and comes with additional ideas on how genetic modifications should be applied \cite{5766165}, such that a good result is computed with higher probability. The Sum of Pairwise Distances can also be used with genetic algorithms. It proposes the idea that a good consensus result can be found by more strongly considering clusterings far apart in the solution space \cite{6694095}. The idea of using Kernel Functions was also proposed \cite{Vega-Pons:2010:WPC:1786814.1787121}, enabling faster computation regardless of the heuristic used, though mostly genetic algorithms are used with this aswell.

Non-negative Matrix Factorization based methods translate the median partitioning problem to a matrix form which can be solved for by running two multiplicative update procedures \cite{Li:2007:SCS:1441428.1442121}. Running those procedures multiple times iteratively improves the result each iteration and the final result can be obtained by stopping the algorithm at any time or whenever it converged.

Binary Linear Programming based methods create a compact representation of the data and model objects on an intermediate level between points and clusters \cite{HUANG2016131}. For those intermediate objects binary decisions on cluster ownership are calculated iteratively using Expectation Maximization (EM). Within the expectation step of EM the factor graph method \cite{HUANG2016131} is used for estimation. The solution is then mapped back from intermediate objects to data points, assigning them their labels for the consensus solution.


\part{Theory}
\chapter{Methods}
The newly created tool uses a large amount of existing methods which are described in this chapter. This description aims to give an overview of what exact methods were used, the theory behind them and in which scenarios they can be applied.

\section{Clustering}
The clustering algorithms implemented were used to create base clusterings for meta-clustering. The algorithms described in this section are the ones used for this step, except for OPTICS which was used for meta-clustering. For additional information on why OPTICS was used on the meta level, see Section [REFFERENCE].


\subsection{OPTICS}

\section{Consensus Clustering}
\subsection{DICLENS}

\section{Other Methods}
\subsection{Hungarians Method}


\part{Implemented Tool}
\chapter{Using the Tool}\label{cha:Tool}
%maybe not "this chapter..."
The implemented tool aims to facilitate the computation and selection of good clustering solutions for a given data-set. To do this both powerful visualizations and algorithms are needed, which are combined here. When working with the created application the user can first import and edit point data, then select clustering algorithms and parameters for meta-clustering and finally use the meta-view to analyze the results or create a consensus results. The final goal after all of this is either having found a clustering solution that is satisfactory or created one using consensus clustering in the final step. This simple workflow can be described with three steps, each with its own view and functionality. In the following sections the views and possibilities within each view are described. For more details on implementation details see Chapter \ref{cha:impl}

\section{Data Visualization and Manipulation}
To start with the tool, the user needs to import or create a data-set on which the clustering should be performed. As the data may have too many dimensions or include unnecessary columns, it needs to be sanitized as well. For this reason, the first window shown implements functionality for both of this steps. When the application is started a scatter plot without data is shown, as well as a side menu with different buttons. This starting window will further on also be called data-view as it visualizes point data, it can be seen in Figure \ref{fig:data-view}. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=.45]{data-view}
	\caption{Data-View, the starting window of the application}
	\label{fig:data-view}
\end{figure}

\subsection{Plots and I/O}

The scatter plot in the center left shows data that was imported or created and allows for changing the shown dimensions by clicking on the center of the corresponding axis. The range of values may also be adjusted this way, clicking on the upper or lower bound of the range on any axis opens a small input box. To automatically set this range, such that the minimum and maximum values of the shown data are just at the border, the small square button in the bottom left can be clicked. This is also a useful button to return to a full view of the data if the range was made smaller to have a zoomed view of the data. The right part of the tool contains the functionality for importing/exporting data, manipulating the data, showing it in a scatter plot matrix and opening the workflow-view which will be explained in the next step.

 Firstly, to import a data-set for visualization and later use the import button in the bottom right can be selected, opening a file dialog which can load either CSV or ARFF files. These formats are supported as they are most commonly used to store such data. This file dialog can be seen in Figure \ref{fig:data-import}. 

\begin{figure}[h]
	\centering 
	\includegraphics[scale=.45]{data-import}%[width=.5\textwidth]
	\caption{Import Dialog}
	\label{fig:data-import}
\end{figure}

After selecting a file the index of cluster labels can be defined, if labels should be loaded from the file. Otherwise the value can be left at $-1$ indicating that no columns should be used for label information. ``$-2$'' may be also be specified, it is used as a shortcut for selecting the last column as cluster labels, removing the need to count columns if it is the last one. The chosen column is then not inserted as a dimension in the data space but used as the cluster labels, which the color of the loaded points will reflect in the scatter plot of the data-view. 

To export data the export button can be used. If will open a similar dialog as for importing, though the index of the labels does not need to be defined, as data with cluster labels will be stored such that the labels for all points will be included in the first column. If the data does not contain label information, this first column is completely omitted. The supported format for exporting data is CSV.

Alternatively, the user can also create a data-set himself from within the tool. It is possible to draw points onto the canvas when the ``SinglePoint'' option is selected in the top right, as in Figure \ref{fig:data-view}. For this, the user only needs to click wherever he wants points placed. It is important to note that if there are more than two dimensions all values for the not shown dimensions will not be set, which may cause a problem for the clustering algorithms later on. Another possibility is the use of the ``ELKIGenerator'', which accepts a XML-style input for defining how to generate data. The definition for this format can be found in corresponding definitions file on the ELKI Github repository \cite{elkixml} or in the tools lib folder. This generator allows defining distributions on dimensions, the number of points for each generated distribution and many additional things like plane rotations. This can be quite useful when wanting to work with a quick sample data-set. In Figure \ref{fig:loaded-data} one can see how the data-view looks when data with cluster labels is shown. 

%%%%%%%%%%%change figure
\begin{figure}[h]
	\centering
	\includegraphics[scale=.45]{data-view}
	\caption{Data-View with data-set (including cluster labels)}
	\label{fig:loaded-data}
\end{figure}

In the top right the ``Matrix'' button can be used to display all dimensions at once, as it will open a new window showing the current data in a scatter plot matrix. Each scatter plot visualizes a different combination of dimensions and since the plots on the diagonal of this view are not useful, they were replaced with kernel density estimation plots. These kernel density estimation plots show how the values of the points in the corresponding dimension are distributed, overlapping the total distribution with the distribution of points with different labels. This may also allow seeing how clusters differ in each dimension.

\subsection{Modifying the data}

As the data may still contain unwanted dimensions after loading cleanup may need to be done. To reduce the dimensionality the simplest method is to select the ``Dim. Remover'' tool in the top right and define the index of the dimension to be removed. As more sophisticated methods PCA and t-SNE [CITATION] are also available. PCA calculates the principal components of the data and projects it onto the $k$ strongest ones leaving us with $k$ dimensions. $k$ can easily be defined in the settings of PCA by filling in the dimensions field, as seen in Figure \ref{fig:loaded-data}. t-SNE works similarly to PCA, has more options though and even a parallel implementation. For more information on those methods see Section [REFERENCE].

Other than reducing the dimensionality of the data the application allows to normalize the data, such that it can be more easily used for clustering. If there are different dimensions with strongly different meaning, unit and range this normalization step can benefit the accuracy of clustering greatly [CITATION?]. The available tools are ``Normalize'' and ``Standardize''. Normalize changes the value range for each dimension to the interval $[0,1]$ while preserving relative distances, the Standardize tool adapts these ranges such that the average is $0$ and the standard deviation is $1$.

With these to pre-processing steps done, the actual clustering of the data can be thought of next. To work on this the ``Clustering'' button opens the next major view of the tool.

\section{Selection of Algorithms and Parameters}
By clicking the ``Clustering'' button in the data-view the window managing clustering workflows, further on called workflow-view, opens. Here clustering algorithms, their parameters and the parameters for meta-clustering can be selected. The workflow-view can be seen in Figure [REFERENCE]

\section{Meta-View and Consensus Clustering}
bla

















\chapter{Implementation}\label{cha:impl}
\section{Programming Language and Tools}
For the implementation of my tool my language of choice was Java. With Java there are lots of available implementations for different clustering methods available, including the Frameworks ELKI \cite{10.1007/978-3-540-69497-7_41} and WEKA \cite{10.1145/1656274.1656278}, as well as the machine learning library Smile \cite{javasmile}. To improve the performance in some parts of the tool Java 1.8 was chosen, making it the lowest required Java version to run it.This was done, as it enables the use of parallel streams which make it simple to parallelize functions. Additionally, it is easy to create custom visualizations using the low level draw() method of JComponents in Swing \cite{javaswing}. All graphs and plots were created using a custom draw() method as this leads to much better performance than using small components for the graphs.

\section{File Input and Output}
Loading and saving data is needed for the tool to make it more usable. For this reason it is possible to not only import data for clustering but also to save workflows and whole groups of clustering results for later reuse.
\subsection{Clustering and Point Data}
Besides being able to create new data sets from within the tool, importing data from CSV or ARFF files is possible. Those file types are supported as they are very common, especially CSV is usually used to store such information. ARFF is commonly used, as it is also the preferred data type of WEKA \cite{10.1145/1656274.1656278}. For this reason the ARFF importer of WEKA was adapted to load in these files such that they can be used with this tool. 

When importing data, it is also possible to define an index of where the cluster labels are located. When this is done the tool will not load the specified column as data but use it to define clusters for the ground truth, which is then also handled differently by the different views of the tool.

The starting window or data-view, in which clusterings can also be loaded back into from the meta-view, also allows for exporting data to CSV. When doing this, the first column in the resulting file will contain the cluster identifiers starting with one. This way computed results can be exported and used in other tools or saved for later.

\subsection{Clustering Workflows}
As many runs of clustering algorithms with different parameter ranges can be performed and repeatedly defining this workflow can be tedious, the tool allows exporting and importing these workflows from the cluster workflow window. For this the underlying clustering objects are serialized and saved into .CWF (Cluster Workflow Files). It is important to note, that these files do not include the parameter settings for random seed or the meta-clustering. This is intentional as I do not consider them to be part of the workflow and they should be set by the user for every run, if the default values are not satisfactory.

\subsection{Pre-Clustered Data}
Due to the clustering algorithms possibly being time consuming, either because of their complexity or just sheer number of runs, the results of these algorithms can be saved from the meta-view. To do this, the clustering results are serialized and saved as .CRF (Clustering Result Files). To load those files one can define the different parameters of the mata-clustering and then import the .CRF. This way, only the mata-clustering is computed again. The mata-clustering is not saved in this file such that the user can more easily try different settings for it, by just saving the result and running the mata-clustering again. As this is usually much faster than the clustering algorithms that generate the base clusterings, using this may save a considerable amount of time.

\section{Methods and Interfaces}
To ensure that new or missing methods can easily be added to the tool, all methods where a selection can be made from within the tool can be added to by implementing onto the corresponding interface. To add methods one needs to first implement it and if needed it's OptionsPanel and then add it to the corresponding static method found in the view using it.

The following sections describe which interfaces and standard methods are implemented, as well as which functions and classes must be edited to add a new custom methods.

\subsection{Data Manipulation}

Data manipulation can be performed as a first step in the data-view to create data-sets or modify them such that they can be used for clustering or visualization. It is also of relevance when loading clustering results back into the data-view to analyze results using dimensionality reduction techniques. For this step three interfaces are of interest:

\begin{itemize}
  \item Generators: IGenerator
  \item Reducers: IReducer
  \item Normalizers: INormalizer
\end{itemize}

%maybe show interface definitions?
\subsubsection{Generators}
Generators allow the user to add or replace data points. Two initial implementations for generators are available, firstly the SinglePointGenerator which allows adding points to the data by clicking the scatter plot. When a point is added this way its coordinates are set to where the user clicked on the screen translated into the coordinates of the data space, such that the point appears below the cursor. Secondly, the ELKIGenerator is available which uses an XML-style input, defining how the clusters should be generated. This class uses the GeneratorXMLDatabaseConnection of ELKI \cite{10.1007/978-3-540-69497-7_41} an is able to create data points according to different random distributions. The definitions file for this input can be found in ELKIs JAR file, their GIT repository \cite{elkixml} or in the /lib folder of the implementation.
%more on this

%implement SamplingReducer?
\subsubsection{Reducers}
Reducers allow the user to manipulate the data such that the dimensionality or potentially (in the future) the number of points can be reduced. Per default three dimensionality reduction techniques are available. The simplest one, DimensionRemover, enables the user to just define a dimension by its index and delete it. This can be useful, if the data is not well cleaned beforehand and irrelevant or possibly disrupting columns are present in the data. For more advanced dimensionality reduction, the PCAReducer and tSNEReducer are available. The PCAReducer, which is adapted from the implementation from Smiles \cite{javasmile} uses Principal Component Analysis [CITATION] to reduce the dimensionality of the data to what the user defined. In contrast the tSNEReducer adapted from Jonsson \cite{javatsne} uses t-Distributed Stochastic Neighbor Embedding [CITATION] and also requires a perplexity parameter.
%perplexity???????

\subsubsection{Normalizers}
Normalizers provide functionality for normalizing the data points. This may be useful whenever different dimensions contain values in strongly differing ranges, while their concrete values do not have any direct relation to the other dimensions. Here, two default implementations are available, Normalize and Standardize. The Normalize class adapts the values such that in each dimension the range is set to the interval $[0,1]$ while preserving relative distances. The Standardize class adapts these ranges such that the average is $0$ and the standard deviation is $1$.
%cite normalize standardize?

\subsubsection*{Adding Data Manipulation Methods}
To additional Methods they must be implemented using the corresponding interface and added to the class DataView. To add them to this class the static methods initGenerators(), initReducers() and initNormalizers() must add the new method to the list of methods. As an example adding a new generator (newGeneratorX) would require adding ``generators.add(new newGeneratorX());'' to the function initGenerators().

\subsection{Clustering Algorithms}
Clustering algorithms can be selected and tuned in the clustering workflow window such that the algorithm is used to create base clustering results for the meta-view. For additional information on the algorithms themselves, see Section [REFERENCE] The following interfaces are relevant for clustering:

\begin{itemize}
  \item Clustering Algorithms: IClusterer
  \item ELKI Clustering Algorithms: IELKIClusterer
  \item Custom Clustering Algorithms: ICustomClusterer
\end{itemize}

\subsubsection{Clustering Algorithms}
Clustering algorithms can be used in the workflow-view to generate base clusterings for mata-clustering and visualization in the meta-view. The interface IClusterer is used as helper interface such that all clustering algorithms can use the same high level calls for compatibility. The actual implementations implement either IELKIClusterer if they use the ELKI database objects in their logic or ICustomClusterer if they use a data matrix. To further help not needing to re-implement generally needed methods the class AbstractClustering should also be extended by the actual implementation. With these class and interfaces the actual implementations can focus more on just the clustering logic. Additionally options panels should be created for each new algorithm allowing the user to set parameters or parameter ranges from the tool.

\subsubsection{ELKI Clustering Algorithms}
IELKIClusterer is used for clustering algorithms that use ELKIs database object to store the data. To run algorithms from ELKI this interface must be implemented and the database object can be used with ELKIs algorithms. The pre-implemented methods include three variants of KMeans, DBScan and EM clustering. All of these methods use ELKIs high level initialization with ``ClassGenericsUtil.parameterizeOrAbort(AlgorithmClass.class, params);'' and other than this only require the logic for managing parameters, random sampling and collection of the result.

\subsubsection{Custom Clustering Algorithms}
To enable other clustering algorithms than the ones implemented in ELKI the interface ICustomClusterer can be used. It receives the two-dimensional data array and the headers as input and allows for the use of any custom clustering logic. One implementation is already available here, namely Spectral Clustering which is implemented with the Smile library \cite{javasmile}.
 
\subsubsection*{Adding Clustering Algorithms}
To add new clustering algorithms for creating base clusterings, first one must choose IELKIClusterer if the method uses ELKIs database object or ICustomClusterer otherwise. This interface then needs to be extended, the AbstractClustering class can help here with utility functions. When extending the corresponding interface an option panel must be implemented and the parameters must be manages within the cluster function. These interfaces also provide functionality for seeded randomness and progress indicators. To properly calculate progress, getCount() must return the proper number of needed executions and cluster() must call addProgress() whenever progress is made. Finally, the newly implemented class must be added to the initClusterers() function in ClusterWorkflow.


\subsection{Meta-Clustering}
%info on OPTICS?
The algorithm used for mata-clustering is OPTICS [CITATION]. One of the requirements for this methods is that the used distance measure must be metric, otherwise the algorithm can only produce an approximate clustering at best. The distance measure used by the algorithm can be changed and additional distance measures can also be added. For this, the following interface is of interest:

\begin{itemize}
  \item Clustering Distance Measure: IMetaDistanceMeasure
\end{itemize}
\subsubsection{Clustering Distance Measures}
The interface IMetaDistanceMeasure is used to define distance measures that can be used for OPTICS for mata-clustering. These distance measures should represent how different two clustering solutions are from each other while needing to fulfill the requirements of a metric. For a distance measure, here indicated with $d(x,y)$ where $x$ and $y$ are objects, to be considered metric the following conditions must hold:
\begin{itemize}
  \item $d(x,y)\geq0$: distances must be non-negative
  \item $d(x,y)=0$ iff $x=y$: the distance between two objects is zero if and only if both objects are equal
  \item $d(x,y)=d(x,y)$: the distance must fulfill the symmetry condition
  \item $d(x,z)\leq d(x,y)+d(y,z)$: the triangle inequality must hold
\end{itemize}

The default implemented distance measures are Variation of Information and Clustering Error. 
%%more
\subsubsection*{Adding Distance Measures}
To add distance measures for OPTICS the interface IMetaDistanceMeasure must be extended and the above mentioned conditions on the distance measure must be fulfilled for the algorithm to produce a satisfying result. After implementing the distance it must be added to initDistances() within ClusterWorkflow, for it to be usable from within the tool.

\subsection{Consensus Clustering}
Consensus Clustering can combine the results form different base clusterings to a common clustering. This can be done in the meta-view and different algorithms can be used for this step. When run, the algorithm computes a consensus result for all base clusterings that are in a common mata-cluster in the OPTICS reachability plot and are not filtered out. This means that if the used defined three mata-clusters with some cut-off value in the reachability plot, there will be three resulting consensus clusterings, one created from each group. The interface of interest for consensus functions is:

\begin{itemize}
  \item Consensus Functions: IConsensusFunction
\end{itemize}

\subsubsection{Consensus Functions}
The consensus functions implement the logic for combining the base clusterings. For this the interface IConsensusFunction is of interest and can be extended to add new functions. Per default two main implementations are available. Both implementations allow for the user to either define a target value for the number of resulting clusters $k$ within each result, or for the algorithm to guess this parameter itself. Firstly, CoAssociationMatrixAverageLink can be used, which computes a Co-Association (CA) Matrix and combines objects via the average link strategy. When $k$ is defined the algorithm joins together sets of points until only $k$ sets remain, which is used as final answer. If no $k$ is defined the algorithm performs two runs, calculating the lifetime \cite{lifetime} of the resulting tree and choosing the level to cut as the one resulting with maximum lifetime. As second method DICLENS is available, which was generously provided by Mimaroglu and Aksehirl \cite{DICLENS}.

\subsubsection*{Adding Consensus Functions}
To add consensus functions the interface IConsensusFunction must be extended. It is also important to note that weights may be supported by the function if implemented, although at this point there is no way for the user to set weights. As for the two default implementations the function may either allow defining the number of resulting clusters per consensus result or not. To add the newly created function to the tool, within MetaViewer the function initConsensusFunctions() must include the new method.

\section{Visualization}

To visualize the data in different ways and provide a functional Graphical User Interface (GUI) the Java Swing \cite{javaswing} Toolkit was used. It allows for both high level calls creating objects withing windows, e.g. panels, buttons or combo-boxes, as well as low level calls by directly changing the behavior of the draw() function. For the basic GUI the panels and buttons of the tool were created with high level calls, while the different graphs and plots were implemented by overriding the draw() function. Overriding the draw() function immensely improved performance as the already available high level objects are computationally costly by comparison, when many of them are needed. This section only includes additional information on the implementation and functionality, information on the usage of the visual elements in the tool can be found in Section [REFERENCE].

\subsection{Scatter Plot}
The scatter plot is the simplest visualization of the data within the tool. It is implemented though two axis and a canvas object. The axis include logic for the draw the value range and selected dimension, including the mouse listeners for modifying it, as well as the translation from model coordinates to viewer coordinates. The canvas implements the drawing of data points, including different shapes and colors depending on their cluster identifier, whether or not they are filtered out or if they represent special objects like the ground truth in the meta-view. Additionally the canvas can support mouse listeners enabling interactivity with the points.
%more?

\subsection{Switching between Clusterings}

bla bla bla

\subsection{Scatter Plot Matrix}

bla bla bla

\subsection{OPTICS Plot}

bla bla bla

\subsection{Heat Map}

bla bla bla

\subsection{Filter Panel}

bla bla bla

\subsection{Cluster comparison}

bla bla bla

\part{Testing}
\chapter{Experiments}\label{cha:experiments}
To show the value of the new tool different scenarios created. The goals of the tool are to improve on the ease of finding complex cluster structures and improve on the result in comparison to only using simple clustering methods. Another aim is to visualize the data and clustering results such that an expert user can make a well informed decision on which solutions best fit the data.
\section{Solutions better than Base-Clusterings}
bla

\section{Multiple Solutions}
bla

\part{Concluding Thoughts}

\chapter{Future Work}\label{cha:futurework}

\section{Improvements for Tool}

bla

\section{Research Consensus Clustering}

bla

\section{Visual Frameworks}

bla


\chapter{Conclusion}\label{cha:conclusion}

\section{Lessons learned}
bla 

\section{Reflection of Work}
In this article I showed 

\begin{appendices}

\chapter{Abstract} % (fold)
\label{cha:abstract}

\section{English abstract} % (fold)
\label{sec:english_abstract}

\input{start/english}

% section english_abstract (end)
\newpage

\section{Deutsche Zusammenfassung} % (fold)
\label{sec:deutsche_zusammenfassung}

\input{start/german}

% section deutsche_zusammenfassung (end)

% section abstract (end)


% chapter abstract (end)


% \include{appendix/acronyms}
% \include{appendix/userGuide}
% \include{appendix/supplementary}

\end{appendices}

\cleardoublepage

\bibliographystyle{acm} 

\bibliography{references}

\printglossaries


\end{document}
