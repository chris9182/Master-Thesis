\documentclass[
	a4paper,
	english,
	twoside,
	openright,               
	11pt                            
	]{report}

\usepackage{textcomp}
\usepackage{listings}
\usepackage{pgfplots}

\usepackage[T1,OT1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{float}
\usepackage{chngcntr}
\usepackage{placeins}
\usepackage[ngerman,american]{babel}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage[justification=centering]{caption}
\usepackage{glossaries}
\graphicspath{ {./img/} }
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\setcounter{tocdepth}{3}
\pgfplotsset{compat=newest}

\begin{document}
\input{titlepage/titlepage}
\input{start/acknowledgements}

\pagenumbering{gobble}

\tableofcontents
 \cleardoublepage
%
%
% list of figures
\listoffigures
%\protect addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\cleardoublepage

% list of tables
\listoftables
%\protect addcontentsline{toc}{chapter}{Tabellenverzeichnis}
\cleardoublepage

% list of algorithms
%\listofalgorithmsLeMATo
%\protect addcontentsline{toc}{chapter}{Algorithmenverzeichnis}
\cleardoublepage

 \part{Introduction and Preliminaries}
 \pagenumbering{arabic}
   \setcounter{page}{1}
 \include{start/introduction}


\part{Theory}

\chapter{Related Work}\label{cha:related_work}
\section{Clustering Algorithms}

A lot of research as been put toward the topic of clustering, resulting in a large number of different methods. \cite{surveyclustering}

\section{Visual Clustering Frameworks}

Clustervision \cite{Kwon2018ClustervisionVS}

VISTA \cite{VISTA} ClusterMap and user result evaluation

iVIBRATE \cite{10.1145/1148020.1148024} extension VISTA?

simple visualizations ELKI implementation \cite{10.14778/2824032.2824115}


Frameworks with specific data-sets in mind:

Propose: ``An Evolutionary and Visual Framework for Clustering of DNA Microarray Data'' \cite{DNAVis}, tool for clustering DNA data with visualization

Even Memes \cite{Dang2017}

\section{Consensus Clustering}

consensus cluster plus \cite{10.1093/bioinformatics/btq170}

\chapter{Methods}

\section{Clustering}
\subsection{OPTICS}

\section{Consensus Clustering}
\subsection{DICLENS}

\section{Other Methods}
\subsection{Hungarians Method}


\part{Implemented Tool}
\chapter{Using the Tool}\label{cha:Tool}
This chapter describes what a user can do with the newly implemented tool. 

\section{I/O and Data Visualization}
bla

\section{Selection of Algorithms and Parameters}
bla

\section{Meta-View and Consensus Clustering}
bla

\chapter{Implementation}
\section{Programming Language and Tools}
For the implementation of my tool my language of choice was Java. With Java there are lots of available implementations for different clustering methods available, including the Frameworks ELKI \cite{10.1007/978-3-540-69497-7_41} and WEKA \cite{10.1145/1656274.1656278}, as well as the machine learning library Smile \cite{javasmile}. To improve the performance in some parts of the tool Java 1.8 was chosen, making it the lowest required Java version to run it.This was done, as it enables the use of parallel streams which make it simple to parallelize functions. Additionally, it is easy to create custom visualizations using the low level draw() method of JComponents in Swing \cite{javaswing}. All graphs and plots were created using a custom draw() method as this leads to much better performance than using small components for the graphs.

\section{File Input and Output}
Loading and saving data is needed for the tool to make it more usable. For this reason it is possible to not only import data for clustering but also to save workflows and whole groups of clustering results for later reuse.
\subsection{Clustering and Point Data}
Besides being able to create new data sets from within the tool, importing data from CSV or ARFF files is possible. Those file types are supported as they are very common, especially CSV is usually used to store such information. ARFF is commonly used, as it is also the preferred data type of WEKA \cite{10.1145/1656274.1656278}. For this reason the ARFF importer of WEKA was adapted to load in these files such that they can be used with this tool. 

When importing data, it is also possible to define an index of where the cluster labels are located. When this is done the tool will not load the specified column as data but use it to define clusters for the ground truth, which is then also handled differently by the different views of the tool.

The starting window, in which clusterings can also be loaded back into from the meta-view, also allows for exporting data to CSV. When doing this, the first column in the resulting file will contain the cluster identifiers starting with one. This way computed results can be exported and used in other tools or saved for later.

\subsection{Clustering Workflows}
As many runs of clustering algorithms with different parameter ranges can be performed and repeatedly defining this workflow can be tedious, the tool allows exporting and importing these workflows from the cluster workflow window. For this the underlying clustering objects are serialized and saved into .CWF (Cluster Workflow Files). It is important to note, that these files do not include the parameter settings for random seed or the meta clustering. This is intentional as I do not consider them to be part of the workflow and they should be set by the user for every run, if the default values are not satisfactory.

\subsection{Pre-Clustered Data}
Due to the clustering algorithms possibly being time consuming, either because of their complexity or just sheer number of runs, the results of these algorithms can be saved from the meta-view. To do this, the clustering results are serialized and saved as .CRF (Clustering Result Files). To load those files one can define the different parameters of the meta clustering and then import the .CRF. This way, only the meta clustering is computed again. The meta clustering is not saved in this file such that the user can more easily try different settings for it, by just saving the result and running the meta clustering again. As this is usually much faster than the clustering algorithms that generate the base clusterings, using this may save a considerable amount of time.

\section{Methods and Interfaces}
To ensure that new or missing methods can easily be added to the tool, all methods where a selection can be made from within the tool can be added to by implementing onto the corresponding interface. To add methods one needs to first implement it and if needed it's OptionsPanel and then add it to the corresponding static method found in the view using it.

The following sections describe which interfaces and standard methods are implemented, as well as which functions and classes must be edited to add a new custom methods.

\subsection{Data Manipulation}

Data manipulation can be performed as a first step in the starting window to create data-sets or modify them such that they can be used for clustering or visualization. It is also of relevance when loading clustering results back into the starting view to analyze results using dimensionality reduction techniques. For this step three interfaces are of interest:

\begin{itemize}
  \item Generators: IGenerator
  \item Reducers: IReducer
  \item Normalizers: INormalizer
\end{itemize}

%maybe show interface definitions?
\subsubsection{Generators}
Generators allow the user to add or replace data points. Two initial implementations for generators are available, firstly the SinglePointGenerator which allows adding points to the data by clicking the scatter plot. When a point is added this way its coordinates are set to where the user clicked on the screen translated into the coordinates of the data space, such that the point appears below the cursor. Secondly, the ELKIGenerator is available which uses an XML-style input, defining how the clusters should be generated. This class uses the GeneratorXMLDatabaseConnection of ELKI \cite{10.1007/978-3-540-69497-7_41} an is able to create data points according to different random distributions. The definitions file for this input can be found in ELKIs JAR file, their GIT repository \cite{elkixml} or in the /lib folder of the implementation.
%more on this

%implement SamplingReducer?
\subsubsection{Reducers}
Reducers allow the user to manipulate the data such that the dimensionality or potentially (in the future) the number of points can be reduced. Per default three dimensionality reduction techniques are available. The simplest one, DimensionRemover, enables the user to just define a dimension by its index and delete it. This can be useful, if the data is not well cleaned beforehand and irrelevant or possibly disrupting columns are present in the data. For more advanced dimensionality reduction, the PCAReducer and tSNEReducer are available. The PCAReducer, which is adapted from the implementation from Smiles \cite{javasmile} uses Principal Component Analysis [CITATION] to reduce the dimensionality of the data to what the user defined. In contrast the tSNEReducer adapted from Jonsson \cite{javatsne} uses t-Distributed Stochastic Neighbor Embedding [CITATION] and also requires a perplexity parameter.
%perplexity???????

\subsubsection{Normalizers}
Normalizers provide functionality for normalizing the data points. This may be useful whenever different dimensions contain values in strongly differing ranges, while their concrete values do not have any direct relation to the other dimensions. Here, two default implementations are available, Normalize and Standardize. The Normalize class adapts the values such that in each dimension the range is set to the interval $[0,1]$ while preserving relative distances. The Standardize class adapts these ranges such that the average is $0$ and the standard deviation is $1$.
%cite normalize standardize?

\subsubsection{Adding Methods}
To additional Methods they must be implemented using the corresponding interface and added to the class StartWindow. To add them to this class the static methods initGenerators(), initReducers() and initNormalizers() must add the new method to the list of methods. As an example adding a new generator (newGeneratorX) would require adding ``generators.add(new newGeneratorX());'' to the function initGenerators().

\subsection{Clustering Algorithms}
Clustering Algorithms can be selected and tuned in the clustering workflow window such that the algorithm is used to create base clustering results for the meta-view. For additional information on the algorithms themselves, see Section [REFERENCE] The following interfaces are relevant for clustering:

\begin{itemize}
  \item Clustering Algorithms: IClusterer
  \item ELKI Clustering Algorithms: IELKIClusterer
  \item Custom Clustering Algorithms: ICustomClusterer
\end{itemize}

\subsubsection{Clustering Algorithms}
The interface IClusterer is used as helper interface such that all clustering algorithms can use the same high level calls for compatibility. The actual implementations implement either IELKIClusterer if they use the ELKI database objects in their logic or ICustomClusterer if they use a data matrix. To further help not needing to re-implement generally needed methods the class AbstractClustering should also be extended by the actual implementation. With these class and interfaces the actual implementations can focus more on just the clustering logic.

\subsubsection{ELKI Clustering Algorithms}

bla bla bla

\subsubsection{Custom Clustering Algorithms}

bla bla bla

\subsubsection{Adding Methods}

bla bla bla


\subsection{Meta-Clustering}
%info on OPTICS?
The algorithm used for meta clustering is OPTICS [CITATION]. One of the requirements for this methods is that the used distance measure must be metric, otherwise the algorithm can only produce an approximate clustering at best. The distance measure used by the algorithm can be changed and additional distance measures can also be added. For this, the following interface is of interest:

\begin{itemize}
  \item Clustering Distance Measure: IMetaDistanceMeasure
\end{itemize}
\subsubsection{Clustering Distance Measure}
The interface IMetaDistanceMeasure is used to define distance measures that can be used for OPTICS for meta clustering. These distance measures should represent how different two clustering solutions are from each other while needing to fulfill the requirements of a metric. For a distance measure, here indicated with $d(x,y)$ where $x$ and $y$ are objects, to be considered metric the following conditions must hold:
\begin{itemize}
  \item $d(x,y)\geq0$: distances must be non-negative
  \item $d(x,y)=0$ iff $x=y$: the distance between two objects is zero if and only if both objects are equal
  \item $d(x,y)=d(x,y)$: the distance must fulfill the symmetry condition
  \item $d(x,z)\leq d(x,y)+d(y,z)$: the triangle inequality must hold
\end{itemize}

The default implemented distance measures are Variation of Information and Clustering Error. 
%%more

\subsection{Consensus Clustering}

bla bla bla

\section{Visualization}

To visualize the data in different ways and provide a functional Graphical User Interface (GUI) the Java Swing \cite{javaswing} Toolkit was used. It allows for both high level calls creating objects withing windows, e.g. panels, buttons or combo-boxes, as well as low level calls by directly changing the behavior of the draw() function. For the basic GUI the panels and buttons of the tool were created with high level calls, while the different graphs and plots were implemented by overriding the draw() function. Overriding the draw() function immensely improved performance as the already available high level objects are computationally costly, when needing many of them, by comparison.

\subsection{Scatter Plot}

bla bla bla

\subsection{Switching between Clusterings}

bla bla bla

\subsection{Scatter Plot Matrix}

bla bla bla

\subsection{OPTICS Plot}

bla bla bla

\subsection{Heat Map}

bla bla bla

\subsection{Filter Panel}

bla bla bla

\subsection{Cluster comparison}

bla bla bla

\part{Testing}
\chapter{Experiments}\label{cha:experiments}
\section{Solutions unattainable from simple Clustering}
bla

\section{Multiple Solutions}
bla

\part{Concluding Thoughts}

\chapter{Future Work}\label{cha:futurework}

\section{Improvements for Tool}

bla

\section{Research Consensus Clustering}

bla

\section{Visual Frameworks}

bla


\chapter{Conclusion}\label{cha:conclusion}

\section{Lessons learned}
bla 

\section{Reflection of Work}
In this article I showed 

\begin{appendices}

\chapter{Abstract} % (fold)
\label{cha:abstract}

\section{English abstract} % (fold)
\label{sec:english_abstract}

\input{start/english}

% section english_abstract (end)
\newpage

\section{Deutsche Zusammenfassung} % (fold)
\label{sec:deutsche_zusammenfassung}

\input{start/german}

% section deutsche_zusammenfassung (end)

% section abstract (end)


% chapter abstract (end)


% \include{appendix/acronyms}
% \include{appendix/userGuide}
% \include{appendix/supplementary}

\end{appendices}

\cleardoublepage

\bibliographystyle{acm} 

\bibliography{references}

\printglossaries


\end{document}
