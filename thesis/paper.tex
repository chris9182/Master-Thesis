\documentclass[
	a4paper,
	english,
	twoside,
	openright,               
	11pt                            
	]{report}

\usepackage{textcomp}
\usepackage{listings}
\usepackage{pgfplots}

\usepackage[T1,OT1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{float}
\usepackage{chngcntr}
\usepackage{placeins}
\usepackage[ngerman,american]{babel}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage[justification=centering]{caption}
\usepackage{glossaries}
\graphicspath{ {./img/} }
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\setcounter{tocdepth}{3}
\pgfplotsset{compat=newest}

\begin{document}
\input{titlepage/titlepage}
\input{start/acknowledgements}

\pagenumbering{gobble}

\tableofcontents
 \cleardoublepage
%
%
% list of figures
\listoffigures
%\protect addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\cleardoublepage

% list of tables
\listoftables
%\protect addcontentsline{toc}{chapter}{Tabellenverzeichnis}
\cleardoublepage

% list of algorithms
%\listofalgorithmsLeMATo
%\protect addcontentsline{toc}{chapter}{Algorithmenverzeichnis}
\cleardoublepage

 \part{Introduction and Preliminaries}
 \pagenumbering{arabic}
   \setcounter{page}{1}
 \include{start/introduction}


\chapter{Related Work}\label{cha:related_work}
\section{Clustering Algorithms}

Clustering is the task of finding groups of points in data such that points within a group have some kind of relationship, while points in different groups do not relate. Defining how such a relationship can be expressed and how these groupings can be found are non-trivial tasks, for which reason a lot of research as been put towards this topic. Different ways of thinking about this problem, the expected results and the performance aspects of the algorithms resulted in a large number of different methods. 

 To summarize which methods are available many surveys were written over time \cite{7154919,1427769,7414675,surveyclustering}. Another aspect of clustering is which kinds of data can be clustered and even for more specific tasks like text clustering many methods are available \cite{5982288}, though these will no be mentioned further on as this paper only works with numeric data. Also, work has been put into optimizing clustering algorithms for different types of architectures and machines, e.g. for distributed systems \cite{6322592} or optimized for energy consumption and load balancing \cite{7586361}. Such aspects will also not be further considered as this paper and the created tool do not aim to beat existing methods in run-time or energy efficiency.

To give an overview over existing clustering algorithms Xu and Tian \cite{surveyclustering} generalize the idea of algorithms to nine categories. These categories are defined by which ideas the algorithms are based on, which are the following:
\begin{itemize}
  \item Partition
  \item Hierarchy
  \item Fuzzy Theory
  \item Distribution
  \item Density
  \item Graph Theory
  \item Grid
  \item Fractal Theory
  \item Model
\end{itemize}

Partition based methods aim to find central points in the data to represent clusters. Data points are then assigned to the central point closes to them and each central point forms a cluster. The most famous method in this group is k-Means which has been implemented in different ways, an example for a newer way for this was proposed by Kanungo et al. \cite{1017616}.

Hierarchy based methods assume at first that each point represents a cluster and merges points to new clusters according to some logic. The opposite can also be done, first assuming all points belong to one cluster and the spiting them into multiple clusters. When starting with each point in one cluster typical linking strategies are single link, joining clusters according to the distance between the closest points, average link, joining according to the average distance of all points from one cluster to the other, or maximum link, whereby the maximum distance between points of the clusters is considered.

Fuzzy Theory based methods do unlike other methods not produce a discrete value of label ownership but a value in the interval $[0,1]$ representing how strongly a point belongs to a cluster. The usual method to compute such an ownership value is to optimize toward an objective function and seeing the values as likelihood of belonging to a given cluster.

Distribution based methods aim to find clusters with the idea that any cluster should have its own mathematical distribution. In other words, each distribution found in the data should have a cluster with its generated points assigned to it. In most cases Gaussian distributions are assumed, reducing the complexity of the search.

Density based methods assume that points with a dense neighborhood should belong to a common cluster. Typical parameters for such clustering algorithms include the size of the neighborhood to consider and the number of points that should be found in their, such that the points are considered densely connected. Example algorithms for this are DBSCAN \cite{10.5555/3001460.3001507} and OPTICS \cite{10.1145/304181.304187}, which was introduced as an improvement of DBSCAN.

Graph Theory based methods work by regarding each point in the data as a vertex of a graph with their distances represent the edges. The algorithms then aim to partition the graph into $k$ sub-graphs each representing a cluster, whereby the edges connecting sub-graphs should have a large sum (of the distances) or small sum (of similarities).

Grid based methods look at the data space as a multidimensional grid. The resulting rectangular spaces are then used in combination with hierarchical and density based methods to bothe improve the result and the computational performance of the algorithms.

Fractal Theory based methods aim to optimize the intrinsic quality of the fractal dimension \cite{surveyclustering}. In this context fractal represents the geometry divisible into multiple parts with common characters with the whole \cite{surveyclustering}.

Model based methods work by first assuming a model for the data and then optimizing the clustering to reflect the choice of model. Many of those are based on statistical learning methods whereby a classification tree is built hierarchically under the assumption that the distribution for each attribute is independent \cite{surveyclustering}.

\subsection{New Clustering Algorithms}

Newer methods for clustering have also been proposed, either with completely novel ideas or by combining ideas from the previous section. To briefly summarize \cite{surveyclustering}, those algorithms are for example based on:

\begin{itemize}
  \item Kernel
  \item Ensemble (more on this in Section \ref{sec:consensus})
  \item Swarm Intelligence
  \item Quantum Theory
  \item Spectral Graph Theory
  \item Affinity Propagation
  \item Density and Distance
\end{itemize}

Additionally algorithms where developed for special data structures \cite{surveyclustering}. These include:

\begin{itemize}
  \item Spatial Data
  \item Data Streams
  \item Large-Scale Data
\end{itemize}

For a more in depth overview Xu and Tian \cite{surveyclustering} published a survey paper, on which the categories in this paper were based on.

%more on current development?
\section{Visual Frameworks}
Visual Frameworks are used to both allow the user to analyze data and empower him to produce better results. Often, when tackling a data analysis problem, the issue is not that there are no good methods available to solve it, but just that the choice of which method produces a satisfying result is difficult. As looking at quality values and data tables does often not reveal the necessary information for educated choices, visualizations and frameworks are needed. Note that for this paper the words framework, tool and application will be used synonymously as I consider them only differing in the way how the user uses the created work, whether it is to be used from code, through APIs or just as a program with graphical user interface.

\subsection{Visual Clustering Frameworks}
Visual clustering frameworks and tools aim to help users to perform clustering in a simpler, more understandable way and to visualize results, such that they can make more educated decisions on which algorithms to use. The visualizations also make it easier to grasp how clusters are structured, which model assumptions can be made and how good the computed result is.

For this reason different frameworks have been proposed in literature, a prominent one of those being Clustervision \cite{Kwon2018ClustervisionVS}. The Clustervision framework allows the user to visually explore the solutions created by multiple clustering algorithm runs, whereby each run may use a different algorithm and different settings for them. Besides just showing the result and information on different properties of the resulting clusters, the framework ranks the computed results according to quality measures. As there exist a large number of different quality measures, each being biased towards/preferring specific cluster structures, the framework uses the average of five different quality measures as an indicator of the quality. A problem of this approach may be that when individual measures are biased and are combined to a more general one, one must be careful to not create new biases by the choice of how to combine them. The choices made by the authors of Clustervision \cite{Kwon2018ClustervisionVS} may for example show favorable results for the presented use-cases but may be poor when facing a data-set that does not fit the assumptions of the chosen metrics. Aside from the problems arising from quality measures, Clustervision only considers clustering solutions that are at least $5\%$ different from other top results, possibly missing better solutions because they are similar to a good result. Also, by choosing a single result through this ranking process the final choice can in the best case be as good as the best produced clustering, even though much more information is available when looking at all solutions. This knowledge can possibly be used to produce a combined result, that may be better than any individual one resulting from simple clustering algorithms. These points are also the reason why the framework created for this thesis was created.

VISTA \cite{VISTA} is another visual clustering framework which was proposed by Chen and Liu and consists of three parts. Firstly, VISTA allows for data filtering which prepares data for visualization. In this step data can be normalized, missing values can be handled and if the dimensionality is to high, reduction techniques can be used. Also, if the data is to large it is possible to reduce it by extracting relevant sub-sets. Next, selecting and filtering of labels is possible. After clustering it is possible extract cluster labels for the points according to the data filtering process, allowing to for example only visualize some clusters. Lastly, the tool allows for post-processing whereby a new cluster presentation is used, which they name ClusterMap. ClusterMap projects the data on a two dimensional grid, showing the cluster labels in each cell and allowing for quick relabeling. Chen and Liu further iterate on this idea by introducing iVIBRATE \cite{10.1145/1148020.1148024} whereby they further analyze the accuracy of their tool and the theoretic background of their method.

Many clustering frameworks also implement visual elements, such that clustering results or data can be explored visually, though this often only includes trivial plots. For example the Java framework ELKI provides such simple functionality \cite{10.14778/2824032.2824115}. Generally, in most programming languages simple scatter plots and charts are available, which can be used whenever no further in-depth analysis is needed.

\subsubsection{Data-set specific Frameworks}
On to of the general frameworks for visualizing clusterings, specific ones exist that aim to better visualize the data as the framework has knowledge on the type of data. 

For example, Castellanos-Garz\'{o}n and D\'{ı}az1 \cite{DNAVis} propose ``An Evolutionary and Visual Framework for Clustering of DNA Microarray Data'' which implements specific visualizations of the data that may not be useful when using other data than DNA data. Still with this knowledge and this specific task these visualizations may prove more useful than only the once targeted at the general use-case of clustering. There are various examples for domains in which such specific frameworks exist, even for the task of clustering ``Memes'' \cite{Dang2017} to detect emerging events and topics driven by rumours, a framework has been created.

\subsection{other Visual Frameworks}

In addition to visual frameworks for clustering, visual parameter space analysis is also relevant, as finding the best parameters for a clustering algorithm may be difficult. For such use-cases Sedlmair et al. \cite{6876043} propose a framework which can help in finding parameters.

There are also few tools that implement additional visualizations for consensus clustering. An example for this is ConsensusClusterPlus \cite{10.1093/bioinformatics/btq170} which visualizes the co-association matrix created from the base clusterings. This matrix represents how often two points occur in a common cluster relative to the number of base clusterings. Looking at this as a heat map a clear block structure is supposed to indicate a good choice for the parameter $k$ which defines the number of clusters in the result partitioning. It also implements Consensus Cumulative Distribution Function (CDF) Plots and Delta Area Plots, showing the CDF of the consensus matrix for each parameter $k$ and relative change for consecutive values of $k$. This Delta Area Plot is supposed to allow the user to determine a value $k$ at which the relative increase is negligible, similar to the elbow-method. Additional plots are also available, further enabling the user to compare the quality of consensus results regarding the parameter $k$.

\section{Consensus Clustering}\label{sec:consensus}
Consensus clustering aims to combine multiple clustering solutions for the same data-set into one result. This result should be representative of the combined knowledge acquired by all base clusterings and therefor be more robust and accurate than the average result at least. As the consensus result should lie in the center of the solution space spanned by the base clusterings, if there are sufficiently many good clusterings it should be able to overcome problems arising from outliers. In literature the problem of consensus clustering is often found under different names, those being cluster ensemble \cite{BOONGOEN20181}, clustering aggregation \cite{Gionis2005ClusteringA} and ensemble of partitions \cite{ensemblepartitions}. For the evaluation of the quality of a result five main factors can be defined \cite{Ghaemi2009ASC,survey1}:\newline

\begin{table}[ht]

\begin{tabular}{ll}
	\textbf{Novelty} & finding results otherwise unattainable \\
	\textbf{Robustness} & having better average quality than input clusterings \\
	\textbf{Consistency}   & finding a result somehow similar to input clusterings \\
	\textbf{Stability} & being more noise resilient than simple clustering algorithms \\
	\textbf{Scalability} & applicability for large data-sets regarding run-time \\
	&
\end{tabular}
\caption{Quality factors for consensus clustering results}
\end{table}

Consensus clustering includes two parts. Firstly base clusterings must be generated, which can be done with different strategies \cite{BOONGOEN20181}. One may choose to perform multiple runs of the same algorithm with the same parameters but different random seeds, the same algorithm with different parameters or even multiple different algorithms. All of these possibilities can be while always clustering the full data, a different sample of the data points (Bootstrapping) \cite{bootstrapping} each run (e.g. 80\%) or even different subspaces of the data. After the generation of base clusterings, the next step is to combine those with a consensus function. In general, consensus functions can be separated into two groups, Object Co-occurrence (O-Co) methods and Median Partitioning (MP) methods \cite{survey1}. There exist a large number of methods in both of those groups, which can be categorized according to what ideas they are based on.

\subsection{Object Co-occurrence}
Object Co-occurrence (O-Co) based consensus functions look at how points relate to each other and uses this information to create a consensus result. If two points often occur in a common cluster of the base clusterings intuition would suggest that they should also appear in a common cluster in the final result. The same holds for points mostly clustered apart, indicating that they should not be clustered together by the consensus function. The different O-Co based methods formulate algorithms that aim to create a result reflecting these thoughts and usually run in a deterministic way, not seeing this task as an optimization problem. This contrasts the methodology of Median Partitioning methods as can be seen in the following section. To summarize the groups of O-Co consensus functions they are categorized by their underlying computational strategy. The different methods are based on:

\begin{itemize}
  \item Relabeling and Voting
  \item Co-association Matrix
  \item Link Uncertainty
  \item Dual-Similarity
  \item Spectral Methods
  \item Graph and Hypergraph
  \item Minimum Spanning Tree
  \item Locally Adaptive Clustering Algorithm
  \item Information Theory
  \item Finite Mixture Model
\end{itemize}

Relabeling and Voting based methods use the individual points as  ``voters'' whereby each point can vote for which cluster it should be assigned to \cite{4470298}. For this to work, the different base clusterings need to map their clusters to one another, defining which cluster is associated most closely with which cluster from another base clustering. This mapping can be performed with algorithms like Kuhn’s Hungarian method \cite{Kuhn2010}.

Co-association Matrix based method create a matrix representing how often two points are in the same cluster relatively  \cite{Monti2003}. With this matrix the consensus result can be computed using linking of sets hierarchically or with a clustering algorithm seeing this matrix as a similarity matrix. The Link Uncertainty \cite{6413733} and Dual-Similarity \cite{7344797} methods expand on this idea by allowing uncertainty values in the matrix and using neighbor-neighbor relationships respectively. Spectral Methods also expand on the idea of co-association matrices by trying to remove noise from the matrix before creating the final result \cite{Tao:2016:RSE:2983323.2983745}.

Graph and Hypergraph based methods create graphs from the base clusterings, representing the relationship between the points as edges and split the resulting graph into sub-graphs with graph partitioning \cite{Strehl:2003:CEK:944919.944935}. Representative methods from this group are the Cluster-based Similarity Partitioning Algorithm (CSPA), the HyperGraph-Partitioning Algorithm (HGPA) and the Meta-CLustering Algorithm (MCLA). Minimum Spanning Tree algorithms require a specific graph structure, organizing them as a minimum spanning tree with Prim's Algorithm \cite[p.~276]{10.1007/978-3-540-85033-5_27}. From this tree the final result is derived through majority voting, a prominent method of this category is Divisive Clustering Ensemble with Automatic Cluster Number (DICLENS) \cite{6035671}.

Locally Adaptive Clustering Algorithm look at different subspaces of the data and computes weights from clustering results in these subspaces \cite{Domeniconi2007}. To create a result the computed clusterings and their weights are modeled as a graph, which is then partitioned.

Information Theory based methods compute quality measures to define distances between clusters and use k-Means to cluster those together \cite[p.~353]{survey1}.

Finite Mixture Model based methods define the consensus clustering problem similarly to the EM-Algorithm (Expectation Maximization) for clustering \cite{Goder2008ConsensusCA,Topchy2004AMM}. The labels for each point are defined as  probability distributions and the final solution is obtained by solving a maximum likelihood estimation problem.

\subsection{Median Partitioning}
The main difference of Median Partitioning in comparison to Object Co-occurrence is that methods of this group tackle consensus clustering as an optimization problem. With the distance between clusterings defined the task of finding a median partition is to look for a solution that would be in the center of the solution space spanned by the base clusterings. In other words the resulting clustering should have a minimal sum of distances to all base clusterings. As this problem has been shown to be NP-hard \cite{np_median_partition}, heuristics must be used. The different groups of methods can be roughly described as methods that are based on:

\begin{itemize}
  \item Genetic Algorithms
  \item Mirkin Distance
  \item Sum of Pairwise Distances
  \item Kernel Functions
  \item Non-negative Matrix Factorization
  \item Binary Linear Programming
\end{itemize}

Genetic Algorithm based methods require a distance measure and a fitness function to compute a consensus result \cite{Cristofor2002FindingMP}. Through evolutionary steps different, starting with random, results are combined or modified until the fitness of the result is satisfactory or does not further improve over many iterations. Mirkin Distance is often used as a distance measure and comes with additional ideas on how genetic modifications should be applied \cite{5766165}, such that a good result is computed with higher probability. The Sum of Pairwise Distances can also be used with genetic algorithms. It proposes the idea that a good consensus result can be found by more strongly considering clusterings far apart in the solution space \cite{6694095}. The idea of using Kernel Functions was also proposed \cite{Vega-Pons:2010:WPC:1786814.1787121}, enabling faster computation regardless of the heuristic used, though mostly genetic algorithms are used with this aswell.

Non-negative Matrix Factorization based methods translate the median partitioning problem to a matrix form which can be solved for by running two multiplicative update procedures \cite{Li:2007:SCS:1441428.1442121}. Running those procedures multiple times iteratively improves the result each iteration and the final result can be obtained by stopping the algorithm at any time or whenever it converged.

Binary Linear Programming based methods create a compact representation of the data and model objects on an intermediate level between points and clusters \cite{HUANG2016131}. For those intermediate objects binary decisions on cluster ownership are calculated iteratively using Expectation Maximization (EM). Within the expectation step of EM the factor graph method \cite{HUANG2016131} is used for estimation. The solution is then mapped back from intermediate objects to data points, assigning them their labels for the consensus solution.


\part{Theory}
\chapter{Methods}
The newly created tool uses a large amount of existing methods which are described in this chapter. This section aims to give an overview of what exact methods were used, the theory behind them and in which scenarios they can be/are applied. For a more general overview of the topics relating the methods used, see the related work for this paper in Chapter \ref{cha:related_work}.

\section{Clustering}
The clustering algorithms implemented were used to create base clusterings for meta-clustering. Besides this, any base clustering result may be chosen by the user as the overall result. The algorithms described here are the ones used in the created tool to generate base clusterings, except for OPTICS which was used for meta-clustering. For additional information on the choice of OPTICS for the meta-level clustering, see Section \ref{sec:sel_alg_param}.

\subsection{k-Means}
bla

\subsection{DBSCAN}
bla

\subsection{Expectation Maximization}
bla

\subsection{OPTICS}
bla

\section{Consensus Clustering}
consensus clustering

\subsection{Co-association Matrix based Consensus}
bla

\subsubsection{Lifetime Criterion}
bla

\subsection{DICLENS}
bla

\section{Other Methods}
\subsection{Dimensionality Reduction}\label{sec:dim_reduction}
PCA \cite{pca}

 t-SNE \cite{Maaten2008VisualizingDU}

\subsection{Hungarians Method}

\subsection{Kernel Density Estimation}

\part{Implemented Tool}
\chapter{Using the Tool}\label{cha:Tool}
%maybe not "this chapter..."
The implemented tool aims to facilitate the computation and selection of good clustering solutions for a given data-set. To do this both powerful visualizations and algorithms are needed, which are combined here. When working with the created application the user can first import and edit point data, then select clustering algorithms and parameters for meta-clustering and finally use the meta-view to analyze the results or create a consensus results. The final goal after all of this is either having found a clustering solution that is satisfactory or created one using consensus clustering in the final step. This simple workflow can be described with three steps, each with its own view and functionality. In the following sections the views and possibilities within each view are described. For more details on implementation details see Chapter \ref{cha:impl}

\section{Data Visualization and Manipulation}
To start with the tool, the user needs to import or create a data-set on which the clustering should be performed. As the data may have too many dimensions or include unnecessary columns, it needs to be sanitized as well. For this reason, the first window shown implements functionality for both of this steps. When the application is started a scatter plot without data is shown, as well as a side menu with different buttons. This starting window will further on also be called data-view as it visualizes point data, it can be seen in Figure \ref{fig:data-view}. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=.45]{data-view}
	\caption{Data-View, the starting window of the application}
	\label{fig:data-view}
\end{figure}

\subsection{Plots and I/O}\label{sec:plotio}

The scatter plot in the center left shows data that was imported or created and allows for changing the shown dimensions by clicking on the center of the corresponding axis. The range of values may also be adjusted this way, clicking on the upper or lower bound of the range on any axis opens a small input box. To automatically set this range, such that the minimum and maximum values of the shown data are just at the border, the small square button in the bottom left can be clicked. This is also a useful button to return to a full view of the data if the range was made smaller to have a zoomed view of the data. The right part of the tool contains the functionality for importing/exporting data, manipulating the data, showing it in a scatter plot matrix and opening the workflow-view which will be explained in the next step.

 Firstly, to import a data-set for visualization and later use the import button in the bottom right can be selected, opening a file dialog which can load either CSV or ARFF files. These formats are supported as they are most commonly used to store such data. This file dialog can be seen in Figure \ref{fig:data-import}. 

\begin{figure}[h]
	\centering 
	\includegraphics[scale=.45]{data-import}%[width=.5\textwidth]
	\caption{Import Dialog}
	\label{fig:data-import}
\end{figure}

After selecting a file the index of cluster labels can be defined, if labels should be loaded from the file. Otherwise the value can be left at $-1$ indicating that no columns should be used for label information. ``$-2$'' may be also be specified, it is used as a shortcut for selecting the last column as cluster labels, removing the need to count columns if it is the last one. The chosen column is then not inserted as a dimension in the data space but used as the cluster labels, which the color of the loaded points will reflect in the scatter plot of the data-view. 

To export data the export button can be used. If will open a similar dialog as for importing, though the index of the labels does not need to be defined, as data with cluster labels will be stored such that the labels for all points will be included in the first column. If the data does not contain label information, this first column is completely omitted. The supported format for exporting data is CSV.

Alternatively, the user can also create a data-set himself from within the tool. It is possible to draw points onto the canvas when the ``SinglePoint'' option is selected in the top right, as in Figure \ref{fig:data-view}. For this, the user only needs to click wherever he wants points placed. It is important to note that if there are more than two dimensions all values for the not shown dimensions will not be set, which may cause a problem for the clustering algorithms later on. Another possibility is the use of the ``ELKIGenerator'', which accepts a XML-style input for defining how to generate data. The definition for this format can be found in corresponding definitions file on the ELKI Github repository \cite{elkixml} or in the tools lib folder. This generator allows defining distributions on dimensions, the number of points for each generated distribution and many additional things like plane rotations. This can be quite useful when wanting to work with a quick sample data-set. In Figure \ref{fig:loaded-data} one can see how the data-view looks when data with cluster labels is shown. 

%%%%%%%%%%%change figure
\begin{figure}[h]
	\centering
	\includegraphics[scale=.45]{data-view}
	\caption{PLACEHOLDER: Data-View with data-set (including cluster labels)}
	\label{fig:loaded-data}
\end{figure}

In the top right the ``Matrix'' button can be used to display all dimensions at once, as it will open a new window showing the current data in a scatter plot matrix. Each scatter plot visualizes a different combination of dimensions and since the plots on the diagonal of this view are not useful, they were replaced with kernel density estimation plots. These kernel density estimation plots show how the values of the points in the corresponding dimension are distributed, overlapping the total distribution with the distribution of points with different labels. This may also allow seeing how clusters differ in each dimension.

\subsection{Modifying the data}

As the data may still contain unwanted dimensions after loading cleanup may need to be done. To reduce the dimensionality the simplest method is to select the ``Dim. Remover'' tool in the top right and define the index of the dimension to be removed. As more sophisticated methods PCA \cite{pca} and t-SNE \cite{Maaten2008VisualizingDU} are also available. PCA calculates the principal components of the data and projects it onto the $k$ strongest ones leaving us with $k$ dimensions. $k$ can easily be defined in the settings of PCA by filling in the dimensions field, as seen in Figure \ref{fig:loaded-data}. t-SNE works similarly to PCA, has more options though and even a parallel implementation. For more information on those methods see Section \ref{sec:dim_reduction}.

Other than reducing the dimensionality of the data the application allows to normalize the data, such that it can be more easily used for clustering. If there are different dimensions with strongly different meaning, unit and range this normalization step can benefit the accuracy of clustering greatly \cite{normalization}. The available tools are ``Normalize'' and ``Standardize''. Normalize changes the value range for each dimension to the interval $[0,1]$ while preserving relative distances, the Standardize tool adapts these ranges such that the average is $0$ and the standard deviation is $1$.

With these to pre-processing steps done, the actual clustering of the data can be thought of next. To work on this the ``Clustering'' button opens the next major view of the tool.

\section{Selection of Algorithms and Parameters}\label{sec:sel_alg_param}
By clicking the ``Clustering'' button in the data-view the window managing clustering workflows, further on called workflow-view, opens. Here clustering algorithms, their parameters and the parameters for meta-clustering can be selected. The workflow-view can be seen in Figure \ref{fig:workflow-view}, there no clustering tasks are added yet and all settings are set to default values.

\begin{figure}[h]
	\centering
	\includegraphics[scale=.43]{workflow-view}
	\caption{Workflow-View with no tasks and default values}
	\label{fig:workflow-view}
\end{figure}

In the top left the selector allows for choosing clustering algorithms to add to the workflow. When an algorithm is selected the corresponding options panel appears in the right allowing to define the parameter ranges that should be sampled. For each parameter the values used for clustering are drawn from a uniform distribution for each run, the number of runs can be defined in the ``Samples'' field. For some algorithms, where there is only the parameter for the number of clusters $k$ as for k-Means, the ``Samples Each'' setting defines how many runs to perform for each value within the range instead of the total number of samples. This is the case as for such methods randomly sampling $k$ is usually not desired, only running the algorithm multiple times for each $k$, as random factors may impact the result. When all parameter ranges and the number of samples are defined, the clustering task can be added to the workflow with the ``Confirm'' button in the bottom right. If a clustering task should be removed, the ``X'' button besides the task can be clicked. An example for the workflow-view with defined clustering tasks can be seen in Figure \ref{fig:workflow-view-tasks}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=.43]{workflow-view-tasks}
	\caption{Workflow-View with tasks defined}
	\label{fig:workflow-view-tasks}
\end{figure}

For ease of use, workflows can be saved and loaded in the workflow-view, usind the ``Save Wf'' and ``Load Wf'' buttons respectively. The tool creates custom Clustering Workflow Files (CWF) for this, allowing to re-use and modify workflows throughout multiple executions of the tool.

\begin{figure}[h]
	\centering
	\includegraphics[scale=.65]{workflow-options}
	\caption{Options for setting Seed and OPTICS parameters}
	\label{fig:workflow-options}
\end{figure}

In the bottom left, see Figure \ref{fig:workflow-options}, additional options are available which are not saved in the CWF files as the user should should set them manually. Defining a random seed by setting the value for ``Seed'' to something other than $-1$ fixes the randomness of the clustering algorithms such that re-running them produces the same result every time, making results reproducible. For meta-clustering the OPTICS \cite{10.1145/304181.304187} algorithm is used as it produces a useful reachability plot in the meta-view, allowing for the identification of dense result regions, possibly indicating robustness. The ``Epsilon'' and ``MinPTS'' parameters allow to define the settings for the OPTICS algorithm, making it possible to define how close clustering results must be to be considered reachable and how many points need to be in this range. With these options, the user is able to define how densely clustering results must occur for them to be shown as robust in the meta-view. The default parameters are chosen such that clusterings are always considered reachable and the neighborhood only considers the closest neighbor for reachability, leading to a DBSCAN-like result. To select the distance function used by OPTICS the selector in the bottom right, just above ``Execute Workflow'' can be used. The ``Execute Workflow'' button is available as soon as at least one clustering task is added.

The additional check boxes in the bottom left allow adding the ground truth to the meta-view (if available) and to manage trivial solutions. Trivial solutions are in this case the solutions in which either all points were clustered into one cluster or each point was assigned its own cluster. One can both delete trivial solutions occurring from the clustering algorithms and forcefully add them to the result. These solutions might prove useful when comparing the NMI value of other solutions to the ground truth, to gain an idea of how good the value is.

When all setting are defined, the user can press the ``Execute Workflow'' button, as visible in Figure \ref{fig:workflow-view}, launching a task that runs the algorithms and opens the meta-view with the computed results. In this step the meta-clustering with OPTICS \cite{10.1145/304181.304187} is also performed.

If the user has a previously saved Clustering Result File (CRF), which can be created in the meta-view, he can also omit defining a workflow and load the results in through the ``Load Result'' button. This button is only available when no clustering tasks were added to the workflow, as can be seen in Figure \ref{fig:workflow-view-tasks}. Before loading a result though, the user should make sure that the settings in the bottom left for meta-clustering are set to his liking. Only the seed field has no effect when loading a result as no clustering algorithms, except for meta-clustering which is deterministic, are run. After the data is loaded and meta-clustering is performed the meta-view opens with the chosen data.

\section{Meta-View and Consensus Clustering}
With either a clustering result loaded in or clustering algorithms run and the meta-clustering performed the meta-view is shown. This view allows the user to comparatively explore the clustering results, here interchangeably the term base clusterings is used, and thereby analyze their robustness. This should empower the used to make educated choices on which clustering to pick as result or to decide on which ones should be combined into a consensus result.













\chapter{Implementation}\label{cha:impl}
\section{Programming Language and Tools}
For the implementation of my tool my language of choice was Java. With Java there are lots of available implementations for different clustering methods available, including the Frameworks ELKI \cite{10.1007/978-3-540-69497-7_41} and WEKA \cite{10.1145/1656274.1656278}, as well as the machine learning library Smile \cite{javasmile}. To improve the performance in some parts of the tool Java 1.8 was chosen, making it the lowest required Java version to run it.This was done, as it enables the use of parallel streams which make it simple to parallelize functions. Additionally, it is easy to create custom visualizations using the low level draw() method of JComponents in Swing \cite{javaswing}. All graphs and plots were created using a custom draw() method as this leads to much better performance than using small components for the graphs.

\section{File Input and Output}
Loading and saving data is needed for the tool to make it more usable. For this reason it is possible to not only import data for clustering but also to save workflows and whole groups of clustering results for later reuse.
\subsection{Clustering and Point Data}
Besides being able to create new data sets from within the tool, importing data from CSV or ARFF files is possible. Those file types are supported as they are very common, especially CSV is usually used to store such information. ARFF is commonly used, as it is also the preferred data type of WEKA \cite{10.1145/1656274.1656278}. For this reason the ARFF importer of WEKA was adapted to load in these files such that they can be used with this tool. 

When importing data, it is also possible to define an index of where the cluster labels are located. When this is done the tool will not load the specified column as data but use it to define clusters for the ground truth, which is then also handled differently by the different views of the tool.

The starting window or data-view, in which clusterings can also be loaded back into from the meta-view, also allows for exporting data to CSV. When doing this, the first column in the resulting file will contain the cluster identifiers starting with one. This way computed results can be exported and used in other tools or saved for later.

\subsection{Clustering Workflows}
As many runs of clustering algorithms with different parameter ranges can be performed and repeatedly defining this workflow can be tedious, the tool allows exporting and importing these workflows from the cluster workflow window. For this the underlying clustering objects are serialized and saved into .CWF (Cluster Workflow Files). It is important to note, that these files do not include the parameter settings for random seed or the meta-clustering. This is intentional as I do not consider them to be part of the workflow and they should be set by the user for every run, if the default values are not satisfactory.

\subsection{Pre-Clustered Data}
Due to the clustering algorithms possibly being time consuming, either because of their complexity or just sheer number of runs, the results of these algorithms can be saved from the meta-view. To do this, the clustering results are serialized and saved as .CRF (Clustering Result Files). To load those files one can define the different parameters of the mata-clustering and then import the .CRF. This way, only the mata-clustering is computed again. The mata-clustering is not saved in this file such that the user can more easily try different settings for it, by just saving the result and running the mata-clustering again. As this is usually much faster than the clustering algorithms that generate the base clusterings, using this may save a considerable amount of time.

\section{Methods and Interfaces}
To ensure that new or missing methods can easily be added to the tool, all methods where a selection can be made from within the tool can be added to by implementing onto the corresponding interface. To add methods one needs to first implement it and if needed it's OptionsPanel and then add it to the corresponding static method found in the view using it.

The following sections describe which interfaces and standard methods are implemented, as well as which functions and classes must be edited to add a new custom methods.

\subsection{Data Manipulation}

Data manipulation can be performed as a first step in the data-view to create data-sets or modify them such that they can be used for clustering or visualization. It is also of relevance when loading clustering results back into the data-view to analyze results using dimensionality reduction techniques. For this step three interfaces are of interest:

\begin{itemize}
  \item Generators: IGenerator
  \item Reducers: IReducer
  \item Normalizers: INormalizer
\end{itemize}

%maybe show interface definitions?
\subsubsection{Generators}
Generators allow the user to add or replace data points. Two initial implementations for generators are available, firstly the SinglePointGenerator which allows adding points to the data by clicking the scatter plot. When a point is added this way its coordinates are set to where the user clicked on the screen translated into the coordinates of the data space, such that the point appears below the cursor. Secondly, the ELKIGenerator is available which uses an XML-style input, defining how the clusters should be generated. This class uses the GeneratorXMLDatabaseConnection of ELKI \cite{10.1007/978-3-540-69497-7_41} an is able to create data points according to different random distributions. The definitions file for this input can be found in ELKIs JAR file, their GIT repository \cite{elkixml} or in the /lib folder of the implementation.
%more on this

%implement SamplingReducer?
\subsubsection{Reducers}
Reducers allow the user to manipulate the data such that the dimensionality or potentially (in the future) the number of points can be reduced. Per default three dimensionality reduction techniques are available. The simplest one, DimensionRemover, enables the user to just define a dimension by its index and delete it. This can be useful, if the data is not well cleaned beforehand and irrelevant or possibly disrupting columns are present in the data. For more advanced dimensionality reduction, the PCAReducer and tSNEReducer are available. The PCAReducer, which is adapted from the implementation from Smiles \cite{javasmile} uses Principal Component Analysis \cite{pca} to reduce the dimensionality of the data to what the user defined. In contrast the tSNEReducer adapted from Jonsson \cite{javatsne} uses t-Distributed Stochastic Neighbor Embedding \cite{Maaten2008VisualizingDU} and also requires a perplexity parameter.
%perplexity???????

\subsubsection{Normalizers}
Normalizers provide functionality for normalizing the data points. This may be useful whenever different dimensions contain values in strongly differing ranges, while their concrete values do not have any direct relation to the other dimensions \cite{normalization}. Here, two default implementations are available, Normalize and Standardize. The Normalize class adapts the values such that in each dimension the range is set to the interval $[0,1]$ while preserving relative distances. The Standardize class adapts these ranges such that the average is $0$ and the standard deviation is $1$.
%cite normalize standardize?

\subsubsection*{Adding Data Manipulation Methods}
To additional Methods they must be implemented using the corresponding interface and added to the class DataView. To add them to this class the static methods initGenerators(), initReducers() and initNormalizers() must add the new method to the list of methods. As an example adding a new generator (newGeneratorX) would require adding ``generators.add(new newGeneratorX());'' to the function initGenerators().

\subsection{Clustering Algorithms}
Clustering algorithms can be selected and tuned in the clustering workflow window such that the algorithm is used to create base clustering results for the meta-view. For additional information on the algorithms themselves, see Section [REFERENCE] The following interfaces are relevant for clustering:

\begin{itemize}
  \item Clustering Algorithms: IClusterer
  \item ELKI Clustering Algorithms: IELKIClusterer
  \item Custom Clustering Algorithms: ICustomClusterer
\end{itemize}

\subsubsection{Clustering Algorithms}
Clustering algorithms can be used in the workflow-view to generate base clusterings for mata-clustering and visualization in the meta-view. The interface IClusterer is used as helper interface such that all clustering algorithms can use the same high level calls for compatibility. The actual implementations implement either IELKIClusterer if they use the ELKI database objects in their logic or ICustomClusterer if they use a data matrix. To further help not needing to re-implement generally needed methods the class AbstractClustering should also be extended by the actual implementation. With these class and interfaces the actual implementations can focus more on just the clustering logic. Additionally options panels should be created for each new algorithm allowing the user to set parameters or parameter ranges from the tool.

\subsubsection{ELKI Clustering Algorithms}
IELKIClusterer is used for clustering algorithms that use ELKIs database object to store the data. To run algorithms from ELKI this interface must be implemented and the database object can be used with ELKIs algorithms. The pre-implemented methods include three variants of KMeans, DBScan and EM clustering. All of these methods use ELKIs high level initialization with ``ClassGenericsUtil.parameterizeOrAbort(AlgorithmClass.class, params);'' and other than this only require the logic for managing parameters, random sampling and collection of the result.

\subsubsection{Custom Clustering Algorithms}
To enable other clustering algorithms than the ones implemented in ELKI the interface ICustomClusterer can be used. It receives the two-dimensional data array and the headers as input and allows for the use of any custom clustering logic. One implementation is already available here, namely Spectral Clustering which is implemented with the Smile library \cite{javasmile}.
 
\subsubsection*{Adding Clustering Algorithms}
To add new clustering algorithms for creating base clusterings, first one must choose IELKIClusterer if the method uses ELKIs database object or ICustomClusterer otherwise. This interface then needs to be extended, the AbstractClustering class can help here with utility functions. When extending the corresponding interface an option panel must be implemented and the parameters must be manages within the cluster function. These interfaces also provide functionality for seeded randomness and progress indicators. To properly calculate progress, getCount() must return the proper number of needed executions and cluster() must call addProgress() whenever progress is made. Finally, the newly implemented class must be added to the initClusterers() function in ClusterWorkflow.


\subsection{Meta-Clustering}
%info on OPTICS?
The algorithm used for mata-clustering is OPTICS \cite{10.1145/304181.304187}. One of the requirements for this methods is that the used distance measure must be metric, otherwise the algorithm can only produce an approximate clustering at best. The distance measure used by the algorithm can be changed and additional distance measures can also be added. For this, the following interface is of interest:

\begin{itemize}
  \item Clustering Distance Measure: IMetaDistanceMeasure
\end{itemize}
\subsubsection{Clustering Distance Measures}
The interface IMetaDistanceMeasure is used to define distance measures that can be used for OPTICS for mata-clustering. These distance measures should represent how different two clustering solutions are from each other while needing to fulfill the requirements of a metric. For a distance measure, here indicated with $d(x,y)$ where $x$ and $y$ are objects, to be considered metric the following conditions must hold:
\begin{itemize}
  \item $d(x,y)\geq0$: distances must be non-negative
  \item $d(x,y)=0$ iff $x=y$: the distance between two objects is zero if and only if both objects are equal
  \item $d(x,y)=d(x,y)$: the distance must fulfill the symmetry condition
  \item $d(x,z)\leq d(x,y)+d(y,z)$: the triangle inequality must hold
\end{itemize}

The default implemented distance measures are Variation of Information and Clustering Error. 
%%more
\subsubsection*{Adding Distance Measures}
To add distance measures for OPTICS the interface IMetaDistanceMeasure must be extended and the above mentioned conditions on the distance measure must be fulfilled for the algorithm to produce a satisfying result. After implementing the distance it must be added to initDistances() within ClusterWorkflow, for it to be usable from within the tool.

\subsection{Consensus Clustering}
Consensus Clustering can combine the results form different base clusterings to a common clustering. This can be done in the meta-view and different algorithms can be used for this step. When run, the algorithm computes a consensus result for all base clusterings that are in a common mata-cluster in the OPTICS reachability plot and are not filtered out. This means that if the used defined three mata-clusters with some cut-off value in the reachability plot, there will be three resulting consensus clusterings, one created from each group. The interface of interest for consensus functions is:

\begin{itemize}
  \item Consensus Functions: IConsensusFunction
\end{itemize}

\subsubsection{Consensus Functions}
The consensus functions implement the logic for combining the base clusterings. For this the interface IConsensusFunction is of interest and can be extended to add new functions. Per default two main implementations are available. Both implementations allow for the user to either define a target value for the number of resulting clusters $k$ within each result, or for the algorithm to guess this parameter itself. Firstly, CoAssociationMatrixAverageLink can be used, which computes a Co-Association (CA) Matrix and combines objects via the average link strategy. When $k$ is defined the algorithm joins together sets of points until only $k$ sets remain, which is used as final answer. If no $k$ is defined the algorithm performs two runs, calculating the lifetime \cite{lifetime} of the resulting tree and choosing the level to cut as the one resulting with maximum lifetime. As second method DICLENS is available, which was generously provided by Mimaroglu and Aksehirl \cite{DICLENS}.

\subsubsection*{Adding Consensus Functions}
To add consensus functions the interface IConsensusFunction must be extended. It is also important to note that weights may be supported by the function if implemented, although at this point there is no way for the user to set weights. As for the two default implementations the function may either allow defining the number of resulting clusters per consensus result or not. To add the newly created function to the tool, within MetaViewer the function initConsensusFunctions() must include the new method.

\section{Visualization}

To visualize the data in different ways and provide a functional Graphical User Interface (GUI) the Java Swing \cite{javaswing} Toolkit was used. It allows for both high level calls creating objects withing windows, e.g. panels, buttons or combo-boxes, as well as low level calls by directly changing the behavior of the draw() function. For the basic GUI the panels and buttons of the tool were created with high level calls, while the different graphs and plots were implemented by overriding the draw() function. Overriding the draw() function immensely improved performance as the already available high level objects are computationally costly by comparison, when many of them are needed. This section only includes additional information on the implementation and functionality, information on the usage of the visual elements in the tool can be found in Chapter \ref{cha:Tool}.

\subsection{Scatter Plot}
The scatter plot is the simplest visualization of the data within the tool. It is implemented though two axis and a canvas object. The axis include logic for the draw the value range and selected dimension, including the mouse listeners for modifying it, as well as the translation from model coordinates to viewer coordinates. The canvas implements the drawing of data points, including different shapes and colors depending on their cluster identifier, whether or not they are filtered out or if they represent special objects like the ground truth in the meta-view. Additionally the canvas can support mouse listeners enabling interactivity with the points.
%more?

\subsection{Switching between Clusterings}

In the meta-view the user is able to change the currently visible clustering by selecting another clustering in the drop-down menu, the heat map or the reachability plot. Whenever this happens the scatter plot showing the current clustering is exchanged and replaces with one showing the selected data. As the labels may be totally different and comparing them in this switching process is visually difficult when they are assigned different colors, Kuhn’s Hungarian method \cite{Kuhn2010} is used to color the points of the newly shown clustering according to the previously shown one. This results in visually similar choices of color for points and thereby allowing for easier comparison.

\subsection{Scatter Plot Matrix}

The scatter plot matrix implements a view that constructs a grid of scatter plots for each combination of dimensions in the data. For the elements on the diagonal of the grid, the plots are exchanged with kernel density estimation plots which are computed using the Java Smile \cite{javasmile} library. Kernel density estimation usually needs a bandwidth parameter to be set which influences how the resulting distributions look. The smile library implements a default value for this parameter though, which estimates a fitting value for this bandwidth. The plot then uses the maximum density for the whole data-set as the $100\% $ mark, drawing it up to the top of the grid cell. On top of this function the functions representing the density for each cluster label are overlayed with different colors and some opacity such that they are all visible. These overlayed functions are assigned their height value by multiplying the density estimation value with the percentage part of the data they contain (e.g. cluster contains $5\% $ of data points $\rightarrow$ height times $0.05$) and scaling these values to screen coordinates, just like the overall function. This results in the function for the whole data representing the sum of the functions for each cluster label.

\subsection{OPTICS Plot}

bla bla bla

\subsection{Heat Map}

bla bla bla

\subsection{Filter Panel}

bla bla bla

\subsection{Cluster comparison}

For the comparison of two clustering results, it is possible to select two clusterings in the meta-view by control-clicking a second clustering when only one is selected. Whenever two clusterings are selected a ``Difference'' button appears, clicking it opens the comparison view. Here, just like when switching between visible clusterings, they are adapted with Kuhn’s Hungarian method \cite{Kuhn2010} such that their common labels are maximal. The center view then colors all points where the labels differ in black and the ones where they are equal in white. This allows to quickly see which points differ for solutions that are at least somewhat similar. The percentage of differing points shown in the title bar of this window is computed by dividing the number of black points by the total number of points.

\part{Testing}
\chapter{Experiments}\label{cha:experiments}
To show the value of the new tool different scenarios created. The goals of the tool are to improve on the ease of finding complex cluster structures and improve on the result in comparison to only using simple clustering methods. Another aim is to visualize the data and clustering results such that an expert user can make a well informed decision on which solutions best fit the data.
\section{Solutions better than Base-Clusterings}
bla

\section{Multiple Solutions}
bla

\part{Concluding Thoughts}

\chapter{Future Work}\label{cha:futurework}

\section{Improvements for Tool}

bla

\section{Research Consensus Clustering}

bla

\section{Visual Frameworks}

bla


\chapter{Conclusion}\label{cha:conclusion}

\section{Lessons learned}
bla 

\section{Reflection of Work}
In this article I showed 

\begin{appendices}

\chapter{Abstract} % (fold)
\label{cha:abstract}

\section{English abstract} % (fold)
\label{sec:english_abstract}

\input{start/english}

% section english_abstract (end)
\newpage

\section{Deutsche Zusammenfassung} % (fold)
\label{sec:deutsche_zusammenfassung}

\input{start/german}

% section deutsche_zusammenfassung (end)

% section abstract (end)


% chapter abstract (end)


% \include{appendix/acronyms}
% \include{appendix/userGuide}
% \include{appendix/supplementary}

\end{appendices}

\cleardoublepage

\bibliographystyle{acm} 

\bibliography{references}

\printglossaries


\end{document}
